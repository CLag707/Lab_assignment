{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formulation:\n",
    "$$ \\mathbf{Z_1} = \\mathbf{X}\\mathbf{W_1} +_r \\mathbf{b_1} $$ \n",
    "represents the approximation function for the first to second layer (X is the input, W is the matrix of weights (dxh1), and b are the biases)\n",
    "$$ \\mathbf{A_1} = \\sigma(\\mathbf{Z_1}) $$\n",
    "Take the sigmoid of the approximation function from input layer to hidden layer, normalizing it \n",
    "$$ \\mathbf{Z_2} = \\mathbf{A_1}\\mathbf{W_2} +_r \\mathbf{b_2} $$\n",
    "Represents approximation from hidden layer to output layer (The input is the approximation function for the first to second layer)\n",
    "$$ \\mathbf{A_2} = \\sigma(\\mathbf{Z_2}) $$\n",
    "Then take the sigmoid of the function\n",
    "$$ \\mathbf{\\hat{y}} = \\mathbf{A_2} $$\n",
    "$$ L = \\frac{1}{2m}||\\mathbf{y}-\\mathbf{\\hat{y}}||^2_2 $$\n",
    "L is the half mean squared error function\n",
    "$$ R = \\frac{\\lambda}{2}(||\\mathbf{W_1}||^2_F + ||\\mathbf{W_2}||^2_F) $$\n",
    "regularization term (lamda controls strength) *penalizes large weights\n",
    "$$ J = L + R $$\n",
    "The cost function\n",
    "\n",
    "Where:\n",
    "* $\\mathbf{X}$ is an $m\\times d$ matrix of training datapoints.\n",
    "* $\\mathbf{W_1}$ and $\\mathbf{W_2}$ are weight matrices with dimensions $d\\times h_1$ and $h_1\\times h_2$, respectively.\n",
    "    * Note that, for all questions in this assignment, $h_2 = 1$.\n",
    "* $\\mathbf{b_1}$ and $\\mathbf{b_2}$ are bias vectors of dim $1\\times h_1$ and $1\\times h_2$, respectively.\n",
    "* $\\sigma(\\mathbf{A})$ is the element-wise sigmoid.\n",
    "* $||\\mathbf{x}||^2_2$ is the square of the vector l2-norm and $||\\mathbf{X}||^2_F$ is the square of the Frobenius norm.\n",
    "* $+_r$ between a matrix and a row vector denotes adding the row vector to every row of the matrix.\n",
    "\n",
    "# Calculate:\n",
    "* $\\frac{\\partial J}{\\partial \\mathbf{W_1}}$\n",
    "* $\\frac{\\partial J}{\\partial \\mathbf{W_2}}$\n",
    "* $\\frac{\\partial J}{\\partial \\mathbf{b_1}}$\n",
    "* $\\frac{\\partial J}{\\partial \\mathbf{b_2}}$\n",
    "\n",
    "# Programming (network):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"A simple function for the sigmoid.\"\"\"\n",
    "    pass\n",
    "\n",
    "def forward(X, W1, b1, W2, b2):\n",
    "    \"\"\"\n",
    "    Do a forward pass on the 3-layer network (defined by two sets of weights/biases) on given data.\n",
    "    Inputs:\n",
    "        X -- m-by-d array, the input data to get the predictions of.\n",
    "        W1 -- d-by-h1 array, the weights of the hidden layer.\n",
    "        b1 -- 1-by-h1 array, the biases of the hidden layer.\n",
    "        W2 -- h1-by-h2 array, the weights of the output layer.\n",
    "        b2 -- 1-by-h2 array, the biases of the output layer.\n",
    "    Returns:\n",
    "        Both layer activations A1 and A2. A2 == y_hat == probabilities of being in class 1.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def get_cost(y, y_hat, W1, W2, LAMBDA):\n",
    "    \"\"\"\n",
    "    Define the cost function as MSE plus l2-regularization term.\n",
    "    Inputs:\n",
    "        y -- m-by-1 array, the training labels (either 0 or 1).\n",
    "        y_hat -- m-by-1 array, the predicted probabilities of being in class 1.\n",
    "        W1 -- d-by-h1 array, the weights of the hidden layer.\n",
    "        W2 -- h1-by-h2 array, the weights of the output layer.\n",
    "        LAMBDA -- the l2-regularization (weights only, not on biases) hyperparameter.\n",
    "    Returns:\n",
    "        The cost value.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def get_classification_error_rate(y, y_hat):\n",
    "    \"\"\"\n",
    "    Compute the classification error rate for a binary classification problem with clases 0 and 1.\n",
    "    Inputs:\n",
    "        y -- m-by-1 array, the training labels (either 0 or 1).\n",
    "        y_hat -- m-by-1 array, the predicted probabilities of being in class 1.\n",
    "    Returns:\n",
    "        The percentage of datapoints misclassified.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def iterate(X, y, W1, b1, W2, b2, ALPHA, LAMBDA):\n",
    "    \"\"\"\n",
    "    Do one gradient descent iteration on given weights/biases for a 3-layer MLP, estimating gradients from given data.\n",
    "    Inputs:\n",
    "        X -- m-by-d array, the training data to estimate gradients with.\n",
    "        y -- m-by-1 array, the training labels (either 0 or 1).\n",
    "        W1 -- d-by-h1 array, the weights of the hidden layer.\n",
    "        b1 -- 1-by-h1 array, the biases of the hidden layer.\n",
    "        W2 -- h1-by-h2 array, the weights of the output layer.\n",
    "        b2 -- 1-by-h2 array, the biases of the output layer.\n",
    "        ALPHA -- the learning rate.\n",
    "        LAMBDA -- the l2-regularization (weights only, not on biases) hyperparameter.\n",
    "    Returns:\n",
    "        The total cost (with regularization penalty) before the update step, as well as the updated weights and biases.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def fit_3_layer_MLP(X, y, MLP_hidden_size, initial_lr=0.1, lr_decay_rate=0.01, max_epochs=10000,\n",
    "                    stopping_threshold=1e-10, l2_lambda=0.0001, print_interval=500, verbosity=1):\n",
    "    \"\"\"\n",
    "    Fit a three-layer MLP on givn data without batching.\n",
    "    Inputs:\n",
    "        X -- m-by-d array, the training data.\n",
    "        y -- m-by-1 array, the training labels (either 0 or 1).\n",
    "        MLP_hidden_size -- the size of the MLP's hidden layer\n",
    "        initial_lr -- the initial learning rate.\n",
    "        lr_decay_rate -- the learning rate decay rate.\n",
    "        max_epochs -- the maximum number of epochs to train for.\n",
    "        stopping_threshold -- if change in cost between epochs goes below this threshold, stop fitting.\n",
    "        l2_lambda -- the l2-regularization (weights only, not on biases) hyperparameter.\n",
    "        print_interval -- every print_interval number of epochs, print training cost if verbosity == 2.\n",
    "        verbose -- 0: no printing, 1: only print termination info, 2: print training loss too.\n",
    "    Returns:\n",
    "        The weight matrices and bias vectors of the fitted MLP, as well as all training costs.\n",
    "    \"\"\"\n",
    "    # run every epoch compute decay-ed learning rate\n",
    "    lr = initial_lr / (1 + lr_decay_rate * (epoch_num+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming (visualization):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after function definition on line 21 (3564845162.py, line 24)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[2], line 24\u001b[1;36m\u001b[0m\n\u001b[1;33m    \"\"\"\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block after function definition on line 21\n"
     ]
    }
   ],
   "source": [
    "def visualize_decision_boundary(neuron_layer, neuron_number, title,\n",
    "                                X_data, y_data, W1, b1, W2, b2,\n",
    "                                plot_datapoints=False, show_rounded=False, granularity=500):\n",
    "    \"\"\"\n",
    "    Plot the decision boundary for a specified neuron in a 3-layer-MLP defined by W1, b1, W2, and b2.\n",
    "    Inputs:\n",
    "        neuron_index -- int, the layer the neuron is in (hidden layer is 0, output layer is 1).\n",
    "        neuron_number -- int, specifies which neuron within the layer to look for (0...layer_size-1).\n",
    "        title -- str, title of the plot.\n",
    "        X_data -- m-by-d array, the data the neuron is trained on.\n",
    "        y_data -- m-by-1 array, training labels.\n",
    "        W1, b1, W2, b2 -- weights of the network from which to select the neuron to visualize.\n",
    "        plot_datapoints -- boolean, whether to plot the X datapoints on top of the heatmap.\n",
    "        show_rounded -- boolean, whether to show prediction instead of probability (whether to round activations).\n",
    "        granularity -- granularity of heatmap.\n",
    "    Returns:\n",
    "        None\n",
    "    Side effects:\n",
    "        Displays heatmap in-line.\n",
    "    \"\"\"\n",
    "    \n",
    "    # create the meshgrid\n",
    "    col_maxes = X_data.max(axis=0)\n",
    "    col_mins = X_data.min(axis=0)\n",
    "    x0_space = np.linspace(start=col_mins[0], stop=col_maxes[0], num=granularity)\n",
    "    x1_space = np.linspace(start=col_mins[1], stop=col_maxes[1], num=granularity)\n",
    "    x0, x1 = np.meshgrid(x0_space, x1_space)\n",
    "    x0 = x0.flatten()\n",
    "    x1 = x1.flatten()\n",
    "    X_grid = np.column_stack((x0,x1))\n",
    "\n",
    "    # get activations for appropriate neuron and organize into grid\n",
    "    A = forward(X_grid, W1, b1, W2, b2)\n",
    "\n",
    "    # get appropriate neuron and round if specified\n",
    "    activations = A[neuron_layer][:,neuron_number]\n",
    "    if show_rounded: activations = np.round(activations)\n",
    "    activations_grid = np.flip(activations.reshape(x1_space.shape[0], x0_space.shape[0]), axis=0)\n",
    "\n",
    "    # plot\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(activations_grid, vmin=0.5, vmax=1, extent=[col_mins[0]-0.1, col_maxes[0]+0.1, col_mins[1]-0.1, col_maxes[1]+0.1])\n",
    "    if plot_datapoints:\n",
    "        positives_mask = np.squeeze(y_data) == 1\n",
    "        negatives_mask = np.squeeze(y_data) == 0\n",
    "        ax.scatter(X_data[positives_mask,0], X_data[positives_mask,1], c=\"tab:orange\", label=\"y = 1\")\n",
    "        ax.scatter(X_data[negatives_mask,0], X_data[negatives_mask,1], c=\"tab:blue\", label=\"y = 0\")\n",
    "        ax.set_xlabel(\"x0\")\n",
    "        ax.set_ylabel(\"x1\")\n",
    "        ax.legend(loc=\"upper right\", bbox_to_anchor=(1.27,1))\n",
    "    ax.set_title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis:\n",
    "* **1**: Use `loadmat()` from `scipy` to load the following datasets:\n",
    "    * `data1.mat`\n",
    "    * `data2.mat`\n",
    "* **2**: Firstly, visualize all data points in a 2-D scatter plot, using a different color for each class. Are the two classes linearly separable?\n",
    "* **3**: Now train an ANN to classify this dataset. It should have two input nodes, two hidden nodes, and one output node. Initialize your weights randomly according to a Gaussian distribution with mean 0 and standard deviation 0.1 (check out the randn function). A suitable value for λ (the regularization coefficient) is 0.0001. You should find a reasonable value for the learning rate, α (i.e., the step size) yourself; using a fixed α should work fine, but feel free to experiment with a dynamic one! You should keep track not only of the cost after each iteration, but also of the classification error (fraction of misclassified data points)2. Because the final output is a real number in [0, 1], you can assign label 1 to outputs greater than 0.5, and 0 to the rest. Choose a suitable maximum number of iterations (or “epochs”) to bring the error to around 5%. After you’ve found a good combination of hyperparameters3 to successfully train the network, plot the resulting cost over time, as well as the training error over time.\n",
    "* **4**: Verify that the trained network is indeed able to correctly classify the training data: using the optimal weights obtained above, make a forward pass with your data, compute the outputs, and assign the corresponding labels. Do they match the correct ones from y train? Make a scatter plot of the data, now with the following color scheme: use the same colors as in part 2-a for correctly labeled points, and a different color for incorrectly labeled ones.\n",
    "* **5**: Visualize the decision boundaries learned by each hidden node in the network. To do this, first you will need to create a 2-D grid of points spanning the region covered by the dataset; you can do that with linspace together with meshgrid after you check the range of the values in the dataset for each of the two coordinates (x1 and x2). Find out what the min and max values are for each coordinate; then, using linspace, create two arrays spanning this range (one for each coordinate); these arrays together define a rectangular region, which meshgrid will populate with a grid of points.\n",
    "* **6**: Now you will “pass” each of the points inside this region through each node in the hidden layer. That is, for the ith node in the hidden layer, compute the dot product of each point (x1, x2) with the weights ($W^{(1)}_{i1}$ , $W^{(1)}_{i2}$) of each node i, add the bias b1 i , and apply the activation\n",
    "function.\n",
    "* **7**: Finally, compute the decision boundary for the output node (the one that will ultimately be used by the network to classify each input). You should proceed analogously as for the hidden nodes, except the two input matrices will now be the outputs from the previous layer (i.e., the two matrices obtained from each of the two hidden nodes). Plot the data points colored according to their true labels on top of this decision surface —- what do you see? Is this what you expected?\n",
    "* **8**: Now we shall look from a different point of view at how the network is capable of classifying these non-linearly separable data points. Visualize the coordinate transformation that takes place after each point passes through the hidden layer: in other words, perform a “half forward pass” with the training data to obtain the a(2) vectors 4 for each data point. Then make a scatter plot, once again coloring each point with their true labels. What\n",
    "happened to the points? Are they (mostly) linearly separable now?\n",
    "* **9**: Note that, since we have two hidden nodes, the a(2) vectors resulting from each data point are still in R2, so there is no dimensionality reduction. How could you describe what is being accomplished by the hidden layer then?\n",
    "* **10**: We will now repeat the analysis above but using a different data set. Load data2.mat and visualize all data points in a 2-D scatter plot, using a different color for each class. Are the two classes linearly separable?\n",
    "* **11**: Train a new ANN to classify this dataset, again using two input nodes, two hidden\n",
    "nodes, and one output node. You should be able to reach a training error around 15% using the same hyperparameters as before.\n",
    "* **12**: Repeat the plots from 2-b to 2-e for the trained network. What do you observe? What is wrong with the decision boundaries?\n",
    "* **13**: Train a new ANN to classify this same dataset, this time using two input nodes, three\n",
    "hidden nodes, and one output node. Repeat the plots from 2-b to 2-e for this network, as well as the plot from 2-f (which should now be a 3-D scatter plot). Comment on the results. What was the effect of adding the third hidden node?\n",
    "* **14**: There is a theorem that says that a 3-layer MLP can approximate any arbitrary decision boundary provided the hidden layer has enough neurons. From the plots above, can you intuitively explain why this theorem is true."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
